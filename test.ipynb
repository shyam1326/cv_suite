{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from time import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The process is running in cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"The process is running in {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset\n",
    "\n",
    "data_transform = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "data_dir = 'data'\n",
    "image_dataset = {\n",
    "    'train': datasets.ImageFolder(os.path.join(data_dir,'train'), transform= data_transform['train']),\n",
    "    'val' : datasets.ImageFolder(os.path.join(data_dir,'test'), transform= data_transform['test'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = {\n",
    "            \"train\": DataLoader(image_dataset[\"train\"], batch_size=16, shuffle=True, num_workers=4),\n",
    "            \"val\": DataLoader(image_dataset[\"val\"], batch_size=16, shuffle=True, num_workers=4)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 640, 640])\n",
      "torch.Size([16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shyam/anaconda3/envs/pytorch_image_classification/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "/Users/shyam/anaconda3/envs/pytorch_image_classification/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fba56a4f1c0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/shyam/anaconda3/envs/pytorch_image_classification/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/Users/shyam/anaconda3/envs/pytorch_image_classification/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/shyam/anaconda3/envs/pytorch_image_classification/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/Users/shyam/anaconda3/envs/pytorch_image_classification/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/Users/shyam/anaconda3/envs/pytorch_image_classification/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/shyam/anaconda3/envs/pytorch_image_classification/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "for x, y in data_loader['train']:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.get_model(\"resnet18\", pretrained=True)\n",
    "\n",
    "classes = image_dataset[\"train\"].classes\n",
    "\n",
    "# Freeze the layers\n",
    "for param in model.parameters():\n",
    "    param.required_grad = False\n",
    "\n",
    "no_of_features = model.fc.in_features\n",
    "model.fc = nn.Linear(no_of_features, len(classes)) #incoming and outgoing features\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "**************************************************\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fba3b83dd50>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m running_accuracy \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(data_loader[phase])\n\u001b[0;32m---> 19\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(data_loader[phase]) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[1;32m     20\u001b[0m     \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m tepoch:\n\u001b[1;32m     21\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch}/{epochs-1}\")\n",
    "    print('*'*50)\n",
    "\n",
    "    for phase in [\"train\", \"val\"]:\n",
    "        training = phase == \"train\"\n",
    "\n",
    "        if training:\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_accuracy = 0.0\n",
    "        print(data_loader[phase])\n",
    "        with tqdm(data_loader[phase]) as tepoch:\n",
    "            for x,y in tepoch:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(training):\n",
    "                    y_pred = model(x)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "                    \n",
    "                    if training:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # Calculate the accuracy\n",
    "                    train_pred = torch.max(y_pred, dim=1).indices\n",
    "                    running_loss += loss.item() * x.size(0)\n",
    "                    running_accuracy += torch.sum(train_pred == y.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset) #len(data_loader[phase])             \n",
    "            epoch_accuracy = running_accuracy.double() / len(data_loader[phase].dataset) #len(data_loader[phase])\n",
    "\n",
    "            print(f\"Epoch: {epoch}, {phase} Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "**************************************************\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mphase\u001b[39m}\u001b[39;00m\u001b[39m Loss: \u001b[39m\u001b[39m{\u001b[39;00mepoch_loss\u001b[39m}\u001b[39;00m\u001b[39m, Accuracy: \u001b[39m\u001b[39m{\u001b[39;00mepoch_accuracy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n\u001b[0;32m---> 45\u001b[0m model \u001b[39m=\u001b[39m train_loop(model, data_loader, loss_fn, optimizer, \u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[19], line 18\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, data_loader, loss_fn, optimizer, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     16\u001b[0m running_accuracy \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(data_loader[phase]) \u001b[39mas\u001b[39;00m tepoch:\n\u001b[1;32m     19\u001b[0m     \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m tepoch:\n\u001b[1;32m     20\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "def train_loop(model, data_loader, loss_fn, optimizer, epochs):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}/{epochs-1}\")\n",
    "        print('*'*50)\n",
    "\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            training = phase == \"train\"\n",
    "\n",
    "            if training:\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_accuracy = 0.0\n",
    "\n",
    "            with tqdm(data_loader[phase]) as tepoch:\n",
    "                for x,y in tepoch:\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(training):\n",
    "                        y_pred = model(x)\n",
    "                        loss = loss_fn(y_pred, y)\n",
    "                        \n",
    "                        if training:\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                        \n",
    "                        # Calculate the accuracy\n",
    "                        train_pred = torch.max(y_pred, dim=1).indices\n",
    "                        running_loss += loss.item() * x.size(0)\n",
    "                        running_accuracy += torch.sum(train_pred == y.data)\n",
    "\n",
    "                epoch_loss = running_loss / len(data_loader[phase].dataset) #len(data_loader[phase])             \n",
    "                epoch_accuracy = running_accuracy.double() / len(data_loader[phase].dataset) #len(data_loader[phase])\n",
    "\n",
    "                print(f\"Epoch: {epoch}, {phase} Loss: {epoch_loss}, Accuracy: {epoch_accuracy}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_loop(model, data_loader, loss_fn, optimizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_suite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
